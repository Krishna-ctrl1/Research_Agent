# ğŸ¤– Autonomous AI Research Agent

An **autonomous AI research assistant** built with **LangChain**, **LangGraph**, and **Ollama**.  
This project takes a research topic, autonomously **creates a plan**, **searches the web**, and **writes a comprehensive report**.  
The entire system is served via a **production-ready FastAPI backend**, **containerized with Docker**, and configured for **automated deployment to AWS**.

---

## ğŸš€ Key Features

- **ğŸ§  Autonomous Multi-Step Research:**  
  The agent plans, searches, critiques, and writes â€” fully autonomously.

- **ğŸ” Stateful Agentic Workflow:**  
  Built using **LangGraph** for complex, cyclical task workflows (planning, self-correction, evaluation).

- **âš¡ Powered by Local LLMs:**  
  Natively integrates with **Ollama** to run open-source models like **Llama 3** directly on your local machine.

- **ğŸ§© Production-Ready API:**  
  Robust and scalable backend built using **FastAPI**, with automatic interactive documentation.

- **ğŸ³ Containerized & Portable:**  
  Pre-configured **Dockerfile** ensures consistent and reproducible environments.

- **ğŸš¢ MLOps Ready:**  
  Includes **GitHub Actions CI/CD pipeline** for automated build and deployment to **AWS App Runner**.

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|------------|-------------|
| **AI Frameworks** | LangChain, LangGraph |
| **LLM Backend** | Ollama (Llama 3) |
| **API Framework** | FastAPI |
| **Search Tool** | DuckDuckGo Search |
| **Containerization** | Docker |
| **CI/CD & Deployment** | GitHub Actions, AWS ECR, AWS App Runner |

---

## ğŸ—ï¸ Project Architecture

```text
+----------------+      +---------------------+      +---------------------+      +----------------------+
| User / Client  |----->|   FastAPI Backend   |----->|  LangGraph Agent    |----->|  DuckDuckGo Search   |
| (e.g., Browser)|      | (api.py)            |      |  (agent.py)         |      |  (Tool)              |
+----------------+      +---------------------+      +----------+----------+      +----------------------+
                                                                 |
                                                                 |
                                                       +---------v---------+
                                                       | Ollama (Llama 3)  |
                                                       | (Local LLM)       |
                                                       +-------------------+
```

## âš™ï¸ Local Setup and Installation

### 1ï¸âƒ£ Prerequisites
Make sure you have the following installed:

- **Python 3.11+**  
- **Git**  
- **Ollama**  
- **Docker Desktop**

---

### 2ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Krishna-ctrl1/Research_Agent.git
cd Research_Agent
```

### 3ï¸âƒ£ Set Up Ollama

Once Ollama is installed and running, pull the **Llama 3** model:

```bash
ollama pull llama3
```

### 4ï¸âƒ£ Configure Environment Variables

Create a .env file in the project root:
```bash
# .env file

# Get these from your LangSmith account settings
LANGCHAIN_TRACING_V2="true"
LANGCHAIN_API_KEY="PASTE_YOUR_LANGSMITH_API_KEY_HERE"
```

### 5ï¸âƒ£ Install Dependencies
```bash
python -m pip install -r requirements.txt
```

## â–¶ï¸ Running the Application

### ğŸ§© Local Development Server

Run the application locally using the following command:

```bash
python main.py
```
**Once started, the API will be available at:**
**ğŸ‘‰ http://127.0.0.1:8000**


## ğŸ§ª Test the API

Visit the interactive documentation:  
ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### Steps:
1. Expand the **POST /research** endpoint  
2. Click **â€œTry it outâ€**  
3. Enter your research topic  
4. Click **â€œExecuteâ€**  

## ğŸ³ Docker Usage

### Build the Docker Image
Build the Docker image using the following command:  
`docker build -t research-agent-api .`

### Run the Docker Container
Run the container with:  
`docker run -p 8000:8000 --env-file .env research-agent-api`

The API will be available at:  
ğŸ‘‰ [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## ğŸš€ MLOps & Deployment (AWS)

This project includes a **CI/CD pipeline** configured for automated deployments using **GitHub Actions** and **AWS App Runner**.

The workflow file is located at:  
`.github/workflows/deploy.yml`

### ğŸ”„ CI/CD Workflow Steps

1. **Configure AWS Credentials:**  
   Authenticates using GitHub Secrets.

2. **Login to ECR:**  
   Authenticates Docker with Amazon Elastic Container Registry (ECR).

3. **Build & Push Image:**  
   Builds and pushes the new Docker image to AWS ECR.

4. **Automatic Deployment:**  
   AWS App Runner automatically detects the new image and deploys it to the live environment.

---

## ğŸ§  Future Enhancements

- Add support for **multi-agent collaboration**  
- Integrate **vector databases** (e.g., Chroma, FAISS) for memory  
- Extend **report export formats** (PDF, DOCX)  
- Introduce a **voice interaction module**

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” feel free to use and modify it.

---

## ğŸ’¡ Author

**Krishna Gupta**  
ğŸ“§ [krishna.gpt607@gmail.com](mailto:krishna.gpt607@gmail.com)  
ğŸŒ [GitHub: Krishna-ctrl1](https://github.com/Krishna-ctrl1)
